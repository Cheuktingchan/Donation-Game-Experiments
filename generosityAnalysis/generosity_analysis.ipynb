{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABPBKoZwHecW",
        "outputId": "22489386-1332-46b5-c9bd-b4bdfbcdced1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deeptime\n",
            "  Downloading deeptime-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from deeptime) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from deeptime) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from deeptime) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1 in /usr/local/lib/python3.10/dist-packages (from deeptime) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1->deeptime) (1.2.0)\n",
            "Installing collected packages: deeptime\n",
            "Successfully installed deeptime-0.4.4\n"
          ]
        }
      ],
      "source": [
        "!pip install deeptime\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "import sys\n",
        "from deeptime.markov.msm import MarkovStateModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuDo4cmlHmpn"
      },
      "source": [
        "#Tuples\n",
        "\n",
        "The core datastructure in our implementation is a tuple. This captures the number of agents with a certain image score. For example, the tuple (2,3,1,0) states that there are 2 agents with an image score of 0, 3 with an image score of 1, 1 with an image score of 2, and none with an image score of 4.\n",
        "\n",
        "#Tuple pairs\n",
        "\n",
        "A tuple pair is used to represent the image scores associated with 2 separate strategies. E.g., ((2,3,1,0),(1,2,0,1)).\n",
        "\n",
        "#Methods\n",
        "\n",
        "We define several utility methods to create, and index tuples and tuple pairs. These depend on set lookups being non-deterministic (which should be the case on the same machine). Methods are as follows.\n",
        "- `array_expand` creates all tuples from a single tuple whose first element is the number of agents in the system. E.g., `array_expand(set([tuple(5,0,0,0)]))` will create (5,0,0,0), (4,1,0,0), (4,0,1,0), ... (0,0,0,5).\n",
        "- `make_indices(num_rep,num_s1,num_s2)` will create all possible tuple pairs given the number of reputations possible in the system `num_rep`, the number of agents allocated to strategy s1 (`num_s1`) and to strategy s2 (`num_s2`).\n",
        "- `make_index(array_set)` given a set of tuple pairs in array_set, this will return two maps, one mapping integers to tuple pairs, and the other mapping tuple pairs to integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKbSsrz8Hl9C"
      },
      "outputs": [],
      "source": [
        "def array_expand(array_set):\n",
        "  \"\"\"creates tuples with the possible combinations of tuples expecting array_set[0] to be the number of agents in the tuple. Example usage: `array_expand(set([tuple(5,0,0,0)]))`\"\"\"\n",
        "  add=set()\n",
        "  finished=False\n",
        "  while not finished:\n",
        "    finished=True\n",
        "    for a in array_set:\n",
        "      for j in range(len(a)-1):\n",
        "        if a[j]!=0:\n",
        "          b=list(a)\n",
        "          b[j]-=1\n",
        "          b[j+1]+=1\n",
        "          b=tuple(b)\n",
        "          if b not in add and b not in array_set:\n",
        "            add.add(b)\n",
        "            finished=False\n",
        "    array_set.update(add)\n",
        "  return array_set\n",
        "\n",
        "def make_indices(num_rep,num_s1,num_s2):\n",
        "  \"\"\"This function splits agents to all possible allocations. E.g., make_indices(6,3,2) will allocate the 3 agents to all possible 6 reputation values in the first tuple and 2 agents to all possible reputation values in the second.\"\"\"\n",
        "  s1a=[0]*num_rep\n",
        "  s2a=[0]*num_rep\n",
        "  s1a[0]=num_s1\n",
        "  s2a[0]=num_s2\n",
        "  i1=array_expand(set([tuple(s1a)]))\n",
        "  i2=array_expand(set([tuple(s2a)]))\n",
        "  return itertools.product(i1,i2)\n",
        "\n",
        "\n",
        "def make_index(array_set):\n",
        "  \"\"\"A simple caching function which allows one to map from ints to tuple pairs and back. returns d,r where d[i] takes an index and returns a tuple and r[a] takes a tuple and returns the index\"\"\"\n",
        "  i=0\n",
        "  d={}\n",
        "  r={}\n",
        "  for a in array_set:\n",
        "    d[i]=a\n",
        "    r[a]=i\n",
        "    i+=1\n",
        "  return (d,r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWF14zcHIpqP"
      },
      "source": [
        "#Modifying tuple pairs\n",
        "\n",
        "The following two methods take in a tuple pair and an index in the pair (with the second tuple in the pair continuing the index).\n",
        "- `increase_image` decreases the element in the index by 1, and increase the element following it by 1 (assuming the index is not at the end of the tuple).\n",
        "- `decrease_image` decreases the element in the index by 1, and increases the preceeding element by 1 (assuming the index is not at the start  of the tuple)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-rIIjLzIqIQ"
      },
      "outputs": [],
      "source": [
        "def increase_image(tup,image_index):\n",
        "  \"\"\"decreases the image at the current image_index, increases it at the next one, unless we're at maximum. Imageindex is an integer between 0 and 2*R-1, i.e., goes across the pair\"\"\"\n",
        "  (n,m)=tup\n",
        "  if image_index==len(n)-1 or image_index==2*len(n)-1:\n",
        "    return tup\n",
        "  n=list(n)\n",
        "  m=list(m)\n",
        "  if image_index<len(n):\n",
        "    n[image_index]-=1\n",
        "    n[image_index+1]+=1\n",
        "  else:\n",
        "    m[image_index-len(n)]-=1\n",
        "    m[image_index-len(n)+1]+=1\n",
        "  return (tuple(n),tuple(m))\n",
        "\n",
        "def decrease_image(tup,image_index):\n",
        "  \"\"\"decreases the image at the current image_index, increases it at the next one, unless we're at maximum. Imageindex is an integer between 0 and 2*R-1, i.e., goes across the pair\"\"\"\n",
        "  (n,m)=tup\n",
        "  if image_index==0 or image_index==len(n):\n",
        "    return tup\n",
        "  n=list(n)\n",
        "  m=list(m)\n",
        "  if image_index<len(n):\n",
        "    n[image_index]-=1\n",
        "    n[image_index-1]+=1\n",
        "  else:\n",
        "    m[image_index-len(n)]-=1\n",
        "    m[image_index-len(n)-1]+=1\n",
        "  return (tuple(n),tuple(m))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UMTsIRWJ8ej"
      },
      "source": [
        "# Eigenvectors\n",
        "\n",
        "We seek to compute the dominant eigenvector of the transition matrix. We use the `deeptime` library for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6MbCh37KMK_"
      },
      "outputs": [],
      "source": [
        "def powermethod(m):\n",
        "  \"\"\"Use the power method to compute the dominant eigenvector\"\"\"\n",
        "  t=MarkovStateModel(m)\n",
        "  return t.stationary_distribution\n",
        "  #Uncomment the following to do this manually, but note that it is more numerically unstable than the code above.\n",
        "  #v=np.random.rand(len(m))\n",
        "  #v=v.astype(np.longdouble)\n",
        "  #for i in range(iters):\n",
        "  #  vp=np.matmul(m,v)\n",
        "  #  vp/=np.sum(vp)\n",
        "  #  (i,max(abs(vp-v))/min(v),max(abs(vp-v)),max(abs(vp-np.matmul(m,v))))\n",
        "  #  nonzeromin=np.min(v[np.nonzero(v)])\n",
        "  #  if max(abs(vp-v))/nonzeromin<=nonzeromin*1e-18:\n",
        "  #    return vp\n",
        "  #  v=vp\n",
        "  #return v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGMw4UddMplf"
      },
      "source": [
        "# Donation probability\n",
        "\n",
        "Consider a donor following a strategy $s_d$, deciding whether to donate to a recipient with and image score $i_r$. Fundamentally, **a donor donates if the image score is equal or greater than the strategy value**. Let\n",
        "- $e_a$ be the _action noise_, the likelihood that even if the agent wanted to donate, they would accidentally not donate.\n",
        "- $e_r$ be the _recall noise_, the likelihood that the donor would incorrectly estimate the recipient's image. If recall noise occurs, the percieved image is drawn uniformly from all possible image scores.\n",
        "- $g$ be the system's _generosity_ value. This is the likelihood that a donor will donate even if they shouldn't.\n",
        "- $r$ is the number of image scores possible.\n",
        "\n",
        "The likelihood of donation is then computed as follows.\n",
        "\n",
        "$$\n",
        "p_{donate}=(1-e_a) \\times\n",
        "  \\begin{cases}\n",
        "  0 \\mbox{ if } s_d > r \\\\\n",
        "  (1-e_r) + e_r(1-\\frac{r-s_d}{r} + g\\frac{r-s_d}{r}) \\mbox{ if } i_r \\geq s_d \\\\\n",
        "  g(1-e_r)+e_r(1-\\frac{r-s_d}{r} + g\\frac{r-s_d}{r}) \\mbox{ otherwise}\n",
        "\\end{cases}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTDMNFEkKO7r"
      },
      "outputs": [],
      "source": [
        "def prob_donate(s_d,i_r,e_a,e_r,g,r):\n",
        "  if s_d>r:\n",
        "    return 0\n",
        "  x=(r-s_d)/r #small speedup: recalculate terms used multiple times, namely (r-s_d)/r\n",
        "  if i_r>=s_d:\n",
        "    return (1-e_a)*((1-e_r)+e_r*(1-x+x*g))\n",
        "  else:\n",
        "    return (1-e_a)*((1-e_r)*g+e_r*(1-x+x*g))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQI4cUMQRF_G"
      },
      "source": [
        "# Building the reputation transition matrix\n",
        "\n",
        "Consider a system with only two strategies $s_1$, $s_2$, and an associated tuple pair $tup=(T_1,T_2)$. We can compute the likelihood of all new tuple pairs being generated following an interaction of these two strategies. To compute this likelihood, we must consider action and recall noise ($e_a, e_r$), generosity ($g$) and the number of image scores possible ($r$).\n",
        "\n",
        "Assume a total of $N$ agents, that the donor follows strategy $s_d$ and has image $i_d$ and recipient follows strategy $s_r$ and has image $i_r$. Then the likelihood of these two agents interacting is\n",
        "$$\n",
        "p_{interact}=\n",
        "\\begin{cases}\n",
        "  \\frac{T_d[i_d]T_r[i_r]}{(N)(N-1)} \\mbox{ if } s_d \\neq s_r \\\\\n",
        "  \\frac{T_d[i_d]^2-T_d[i_d]}{(N)(N-1)} \\mbox{ otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Here e.g., $T_d[i_d]$ denotes the number of agents at index $i_d$ in the tuple associated with strategy $s_d$.\n",
        "\n",
        "We iterate over all possible tuples setting their probability to $p_{interact}p_{donate}$ if the image incremented, and to $p_{interact}(1-p_{donate})$ if the image is decremented.\n",
        "\n",
        "The method `compute_transitions` takes in a tuple, strategies, noise and generosity parameters, and returns a map of new tuple $\\to$ probabilities for non-zero likelihoods.\n",
        "\n",
        "By calling this for all possible tuples, we can create the transition matrix. This is done by the `make_transition_matrix` which creates and indexes the tuples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQKu4dL_PZZz"
      },
      "outputs": [],
      "source": [
        "def compute_transitions(tup,s1,s2,e_a,e_r,g):\n",
        "  \"\"\"takes in a ([n_1, ..., n_r],[m_1, ... m_r]) tuple and returns a [new_tuple]->likelihood tuple for those transitions\"\"\"\n",
        "  probabilities={}\n",
        "\n",
        "  (n,m)=tup\n",
        "  num_agents=sum(n)+sum(m)\n",
        "  for donor in range(len(n)+len(m)):\n",
        "    for recipient in range(len(n)+len(m)):\n",
        "      if (n+m)[donor]==0 or (n+m)[recipient]==0: #speedup: if there is no donor or recipient agent at this index, just continue.\n",
        "        continue\n",
        "      #donor and recipient are indices\n",
        "      prob_play=0\n",
        "      if donor!=recipient:\n",
        "        prob_play=(n+m)[donor]*(n+m)[recipient]/(num_agents*(num_agents-1))\n",
        "      else:\n",
        "        prob_play=((n+m)[donor]**2-(n+m)[donor])/(num_agents*(num_agents-1))\n",
        "      #we now have the likelihood of them playing against each other\n",
        "      #now get the actual indices to work out the strategies.\n",
        "      if donor-len(n)<0:\n",
        "        strat1=s1\n",
        "      else:\n",
        "        strat1=s2\n",
        "      if recipient-len(n)<0:\n",
        "        strat2=s1\n",
        "      else:\n",
        "        strat2=s2\n",
        "      image1=donor%len(n)\n",
        "      image2=recipient%len(n)\n",
        "\n",
        "      p_donate=prob_donate(strat1,image2,e_a,e_r,g,len(n))\n",
        "      donate_tuple=increase_image(tup,donor)\n",
        "      no_donate_tuple=decrease_image(tup,donor)\n",
        "\n",
        "      probabilities[donate_tuple]=p_donate*prob_play+probabilities.get(donate_tuple,0)\n",
        "      probabilities[no_donate_tuple]=(1-p_donate)*prob_play+probabilities.get(no_donate_tuple,0)\n",
        "  return probabilities\n",
        "\n",
        "def make_transition_matrix(num_s1,num_s2,s1,s2,e_a,e_r,g,r):\n",
        "  tuples=list(make_indices(r,num_s1,num_s2))\n",
        "  (index_to_tuple,tuple_to_index)=make_index(tuples)\n",
        "  transition_matrix=np.zeros([len(tuples),len(tuples)])\n",
        "  for t in tuples:\n",
        "    transition_probs=compute_transitions(t,s1,s2,e_a,e_r,g)\n",
        "    for p in transition_probs:\n",
        "      transition_matrix[tuple_to_index[t],tuple_to_index[p]]=transition_probs[p]\n",
        "  return transition_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GYtE6OhYwd5"
      },
      "source": [
        "# Computing Expected Utilities\n",
        "\n",
        "Given a tuple pair, associated strategies, noise and generosity parameters, we can compute the expected utility of each strategy by looking at their likelihood of interacting as a donor or recipient, the probability of a donation taking place, and the utility of donation or non-donation to them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYX6X64GZ9X4"
      },
      "outputs": [],
      "source": [
        "def tuple_utility(tup,s1,s2,donate_util,recieve_util,e_a,e_r,g):\n",
        "  \"\"\"this returns a pair of utilities for each strategy by considering the likelihood that a pair of agents play each other and multiplying that by the likelihood of donating and the payoff for donating\"\"\"\n",
        "  util={s1:0,s2:0}\n",
        "  (n,m)=tup\n",
        "  num_agents=sum(n)+sum(m)\n",
        "  for donor in range(len(n)+len(m)):\n",
        "    for recipient in range(len(n)+len(m)):\n",
        "      if (n+m)[donor]==0 or (n+m)[recipient]==0: #speedup: if there is no donor or recipient agent at this index, just continue.\n",
        "        continue\n",
        "      #donor and recipient are indices\n",
        "      prob_play=0\n",
        "      if donor!=recipient:\n",
        "        prob_play=(n+m)[donor]*(n+m)[recipient]/(num_agents*(num_agents-1))\n",
        "      else:\n",
        "        prob_play=((n+m)[donor]**2-(n+m)[donor])/(num_agents*(num_agents-1))\n",
        "      #we now have the likelihood of them playing against each other\n",
        "      #now get the actual indices to work out the strategies.\n",
        "      if donor-len(n)<0:\n",
        "        strat1=s1\n",
        "      else:\n",
        "        strat1=s2\n",
        "      if recipient-len(n)<0:\n",
        "        strat2=s1\n",
        "      else:\n",
        "        strat2=s2\n",
        "      image1=donor%len(n)\n",
        "      image2=recipient%len(n)\n",
        "\n",
        "      p_donate=prob_donate(strat1,image2,e_a,e_r,g,len(n))\n",
        "      util[strat1]+=p_donate*prob_play*donate_util\n",
        "      util[strat2]+=p_donate*prob_play*recieve_util\n",
        "  return util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7OdCg3Ba9NL"
      },
      "source": [
        "We can also compute the utility of a group of agents where $|s_1|$ agents follow strategy $s_1$ and $|s_2|$ follow strategy $s_2$. For this, we need to compute the likelihood of each tuple arising, and multiply it by the utility computed above. The likelihood of the tuple arising is obtained by the vector of the stationary distribution of the transition matrix (computed via the power method).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sVpQ2FHaVtU"
      },
      "outputs": [],
      "source": [
        "def compute_utility(num_s1,num_s2,s1,s2,e_a,e_r,g,r,donate_util,recieve_util):\n",
        "  \"\"\"computes the utility for a specific distribution of s1 and s2 strategies across all tuples with that number\"\"\"\n",
        "  tm=make_transition_matrix(num_s1,num_s2,s1,s2,e_a,e_r,g,r)\n",
        "  vec=powermethod(tm)\n",
        "  util={s1:0,s2:0}\n",
        "  tuples=list(make_indices(r,num_s1,num_s2))\n",
        "  (index_to_tuple,tuple_to_index)=make_index(tuples)\n",
        "  for i in range(len(vec)):\n",
        "    t=index_to_tuple[i]\n",
        "    u=tuple_utility(t,s1,s2,donate_util,recieve_util,e_a,e_r,g)\n",
        "    util[s1]+=u[s1]*vec[i]\n",
        "    util[s2]+=u[s2]*vec[i]\n",
        "  return util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwStNxi0b2dr"
      },
      "source": [
        "# From reputation to evolution dynamics\n",
        "\n",
        "We assume that agents learn strategies on a different, and much slower, timescale than image scores get updated.\n",
        "\n",
        "We treat strategy learning as a Fermi process. Here, a \\emph{focal agent}, which may learn a new strategy, is picked from the population at random. If this agent follows a strategy $s_1$, it'll change it's strategy to $s_2$ with likelihood\n",
        "\n",
        "$$\\frac{1}{1+e^{-\\beta(\\pi_2-\\pi_1)}}$$\n",
        "\n",
        "where $\\pi_2$ and $\\pi_1$ are the expected utilities of strategy $s_2$ and $s_1$ respectively. The $\\beta$ parameter controls the likelihood of imitation. A large $\\beta$ makes adoption of a higher utility strategy more likely, while a small $\\beta$ increases the likelihood of a random strategy being adopted.\n",
        "\n",
        "We assume that our population of agents contains a single strategy for nearly all time periods, and is occassionaly invaded by one agent pursuing a different strategy. The _fixation probability_ computes the likelihood of this new strategy taking over the entire population as agents learn new behaviours. When a Fermi process is used for learning, the fixation probability of a strategy $s_2$ taking over from a strategy $s_1$ for a population of $N$ agents is computed as follows.\n",
        "\n",
        "$$p_{fix}(s_1,s_2) = \\frac{1}{1+ \\sum_1^{N-1}\\Pi_{k=1}^i e^{-\\beta(\\pi_2 - \\pi_1)}}$$\n",
        "\n",
        "We can define a matrix whose non-diagonal elements are $kp_{fix}(s_1,s_2)$ and whose diagonal elements (for strategy $s_1$) are $1-k\\sum_s p_{fix}(s_1,s)$ where $k$ is a sufficiently small constant to ensure that all elements are positive. A simple value for $k$ is $1/|S|$ where $|S|$ is the number of strategies in the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvHeVUmOe8ZD"
      },
      "outputs": [],
      "source": [
        "def strat_transition_matrix(num_agents,e_a,e_r,g,r,donate_util,recieve_util,beta):\n",
        "  fm=np.zeros([r+1,r+1])\n",
        "  for i in range(r+1):\n",
        "    for j in range(r+1):\n",
        "      s1=i\n",
        "      s2=j\n",
        "      sm=0\n",
        "      for na in range(1,num_agents):\n",
        "        tmp=1\n",
        "        for k in range(1,na+1):\n",
        "          u=compute_utility(k,num_agents-k,s1,s2,e_a,e_r,g,r,donate_util,recieve_util)\n",
        "          us1=u[s1]\n",
        "          us2=u[s2]\n",
        "\n",
        "          tmp*=math.exp(-beta*(us2-us1)) #faster than calling fermi_learning, constant is the beta parameter in fermi\n",
        "        sm+=tmp\n",
        "      fixation=1/(1+sm)\n",
        "      fm[i,j]=fixation\n",
        "  fm/=r\n",
        "  for i in range(r+1):\n",
        "    fm[i,i]=1-(sum(fm[i])-fm[i,i])\n",
        "  return fm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I83hpe9Rf_NF"
      },
      "source": [
        "# Cooperation index\n",
        "\n",
        "The stationary distribution of the matrix described above captures the fraction of time that each strategy exists alone in the system.\n",
        "\n",
        "The _cooperation index_ of a system is a number between 0 and 1 denoting the fraction of cooperative actions which take place when a system is stable. We can compute this by considering the likelihood of being in some state (i.e., strategy), multiplied by the likelihood of a cooperative action (i.e., a donation) taking place. This latter likelihood in turn requires us to consider the likelihood of each image score tuple pair when agents follow that strategy, and the likelihood of donation in that case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWOS_FCKhC39"
      },
      "outputs": [],
      "source": [
        "def cooperation_index(num_agents,e_a,e_r,g,r,donate_util,recieve_util,beta):\n",
        "  \"\"\"The cooperation index is calculated as the likelihood of ending up in a single agent state (c.f., the strat_transition_matrix)\n",
        "     times the likelihood of having some distribution of images in that state times the likelihood that an agent will donate in that state.\"\"\"\n",
        "  sum=0\n",
        "  stm=strat_transition_matrix(num_agents,e_a,e_r,g,r,donate_util,recieve_util,beta)\n",
        "  p=powermethod(stm)\n",
        "\n",
        "  for i in range(len(p)):\n",
        "    tm=make_transition_matrix(num_agents,0,i,0,e_a,e_r,g,r) #we can consider all agents being in the first tuple.\n",
        "    vec=powermethod(tm)\n",
        "    tuples=list(make_indices(r,num_agents,0))\n",
        "    (index_to_tuple,tuple_to_index)=make_index(tuples)\n",
        "\n",
        "    for j in range(len(vec)):\n",
        "      current_tuple=index_to_tuple[j][0]\n",
        "      probability_of_tuple=vec[j]\n",
        "\n",
        "      for donor in range(len(current_tuple)):\n",
        "        for recipient in range(len(current_tuple)):\n",
        "          if current_tuple[donor]==0 or current_tuple[recipient]==0:\n",
        "            continue\n",
        "          prob_play=0\n",
        "          if donor!=recipient:\n",
        "            prob_play=current_tuple[donor]*current_tuple[recipient]/(num_agents*(num_agents-1))\n",
        "          else:\n",
        "            prob_play=(current_tuple[donor]**2-current_tuple[donor])/(num_agents*(num_agents-1))\n",
        "\n",
        "          image1=donor\n",
        "          image2=recipient\n",
        "\n",
        "          p_donate=prob_donate(i,image2,e_a,e_r,g,r)\n",
        "          sum+=p_donate*prob_play*probability_of_tuple*p[i]\n",
        "  return sum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZXf9OZ3vfz3"
      },
      "source": [
        "#Experiments\n",
        "Below this point we move to generating results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhWGYcXTih0U"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=8,suppress=True,threshold=sys.maxsize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCUFKPScZX3E"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "\n",
        "linestyles_dict = OrderedDict(\n",
        "    [('solid',               (0, ())),\n",
        "     ('loosely dotted',      (0, (1, 10))),\n",
        "     ('dotted',              (0, (1, 5))),\n",
        "     ('densely dotted',      (0, (1, 1))),\n",
        "\n",
        "     ('loosely dashed',      (0, (5, 10))),\n",
        "     ('dashed',              (0, (5, 5))),\n",
        "     ('densely dashed',      (0, (5, 1))),\n",
        "\n",
        "     ('loosely dashdotted',  (0, (3, 10, 1, 10))),\n",
        "     ('dashdotted',          (0, (3, 5, 1, 5))),\n",
        "     ('densely dashdotted',  (0, (3, 1, 1, 1))),\n",
        "\n",
        "     ('loosely dashdotdotted', (0, (3, 10, 1, 10, 1, 10))),\n",
        "     ('dashdotdotted',         (0, (3, 5, 1, 5, 1, 5))),\n",
        "     ('densely dashdotdotted', (0, (3, 1, 1, 1, 1, 1)))])\n",
        "\n",
        "colors_dict = OrderedDict(\n",
        "    [('1', '#ff9933'),\n",
        "     ('2', '#009999'),\n",
        "     ('3', '#cc66ff'),\n",
        "     ('4', '#6699ff'),\n",
        "     ('5', '#ff9900'),\n",
        "     ('6', '#009933')])\n",
        "\n",
        "lines_dict = OrderedDict(\n",
        "     [('1', (0, (1, 5))),\n",
        "     ('2', (0, (5, 5))),\n",
        "     ('3', (0, (3, 5, 1, 5))),\n",
        "     ('4', (0, (1, 10))),\n",
        "     ('5', (0, (5, 10))),\n",
        "     ('6', (0, (3, 10, 1, 10, 1, 10))),\n",
        "     ('7', (0, (1, 1))),\n",
        "     ('8', (0, (5, 1))),\n",
        "     ('9', (0, (3, 1, 1, 1)))])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCJv8H-IZcKR"
      },
      "outputs": [],
      "source": [
        "#figure 4a\n",
        "fig=plt.figure()\n",
        "plt.xlim(-0.02,0.22)\n",
        "plt.ylim(-0.02,1.2)\n",
        "plt.ylabel(\"Cooperation index\")\n",
        "plt.xlabel(\"Action and reputation noise\")\n",
        "\n",
        "xpoints = [0.0, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2]\n",
        "\n",
        "ypoints1=[]\n",
        "ypoints2=[]\n",
        "ypoints3=[]\n",
        "for x in xpoints:\n",
        "  ypoints1.append(cooperation_index(6,x,x,0,5,-0.1,1.0,10))\n",
        "  ypoints2.append(cooperation_index(6,x,x,0.01,5,-0.1,1.0,10))\n",
        "  ypoints3.append(cooperation_index(6,x,x,0.05,5,-0.1,1.0,10))\n",
        "\n",
        "plt.plot(xpoints,ypoints1,color=colors_dict['1'], marker='o', linestyle=lines_dict['1'],\n",
        "         label=\"g2=0\")\n",
        "plt.plot(xpoints,ypoints2,color=colors_dict['2'], marker='o', linestyle=lines_dict['2'],\n",
        "         label=\"g2=0.01\")\n",
        "plt.plot(xpoints,ypoints3,color=colors_dict['3'], marker='o', linestyle=lines_dict['3'],\n",
        "         label=\"g2=0.05\")\n",
        "plt.legend(loc='upper right')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#figure 4b\n",
        "fig=plt.figure()\n",
        "plt.xlim(1.98,7.02)\n",
        "plt.ylim(-0.02,1.2)\n",
        "plt.ylabel(\"Cooperation index\")\n",
        "plt.xlabel(\"Num. agents\")\n",
        "\n",
        "xpoints = [2, 3, 4, 5, 6, 7]\n",
        "\n",
        "ypoints1=[]\n",
        "ypoints2=[]\n",
        "ypoints3=[]\n",
        "for x in xpoints:\n",
        "  ypoints1.append(cooperation_index(x,0.025,0.025,0,5,-0.1,1.0,10))\n",
        "  ypoints2.append(cooperation_index(x,0.025,0.025,0.01,5,-0.1,1.0,10))\n",
        "  ypoints3.append(cooperation_index(x,0.025,0.025,0.05,5,-0.1,1.0,10))\n",
        "\n",
        "plt.plot(xpoints,ypoints1,color=colors_dict['1'], marker='o', linestyle=lines_dict['1'],\n",
        "         label=\"g2=0\")\n",
        "plt.plot(xpoints,ypoints2,color=colors_dict['2'], marker='o', linestyle=lines_dict['2'],\n",
        "         label=\"g2=0.01\")\n",
        "plt.plot(xpoints,ypoints3,color=colors_dict['3'], marker='o', linestyle=lines_dict['3'],\n",
        "         label=\"g2=0.05\")\n",
        "plt.legend(loc='upper right')"
      ],
      "metadata": {
        "id": "y1DycB0_cBY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#figure 4c num_agents,e_a,e_r,g,r,donate_util,recieve_util,beta\n",
        "fig=plt.figure()\n",
        "plt.xlim(1.98,8.02)\n",
        "plt.ylim(-0.02,1.2)\n",
        "plt.ylabel(\"Cooperation index\")\n",
        "plt.xlabel(\"Poss. image scores\")\n",
        "\n",
        "xpoints = [2, 3, 4, 5, 6, 7,8]\n",
        "\n",
        "ypoints1=[]\n",
        "ypoints2=[]\n",
        "ypoints3=[]\n",
        "for x in xpoints:\n",
        "  ypoints1.append(cooperation_index(4,0.025,0.025,0,x,-0.1,1.0,10))\n",
        "  ypoints2.append(cooperation_index(4,0.025,0.025,0.01,x,-0.1,1.0,10))\n",
        "  ypoints3.append(cooperation_index(4,0.025,0.025,0.05,x,-0.1,1.0,10))\n",
        "\n",
        "plt.plot(xpoints,ypoints1,color=colors_dict['1'], marker='o', linestyle=lines_dict['1'],\n",
        "         label=\"g2=0\")\n",
        "plt.plot(xpoints,ypoints2,color=colors_dict['2'], marker='o', linestyle=lines_dict['2'],\n",
        "         label=\"g2=0.01\")\n",
        "plt.plot(xpoints,ypoints3,color=colors_dict['3'], marker='o', linestyle=lines_dict['3'],\n",
        "         label=\"g2=0.05\")\n",
        "plt.legend(loc='upper right')"
      ],
      "metadata": {
        "id": "5tj_tJHRcg4R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
